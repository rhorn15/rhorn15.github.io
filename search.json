[
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Ryan Horn’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHoliday Movies\n\n\n\n\n\n\n\n\nApr 9, 2025\n\n\nRyan Horn\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify Analytics\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\nRyan Horn\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nPython Basics\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nRyan Horn\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nRyan Horn\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nRestaurant Inspections\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nRyan Horn\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nNFL 2022 Data\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nRyan Horn\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nBeer Markets\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nRyan Horn\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pandas_basics.html#creating-a-series",
    "href": "pandas_basics.html#creating-a-series",
    "title": "Pandas Basics",
    "section": "Creating a Series",
    "text": "Creating a Series\n\n\n# Creating a Series from a list\ndata = [10, 20, 30, 40, 50]\nseries = pd.Series(data)\nseries\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30\n\n\n3\n40\n\n\n4\n50\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "pandas_basics.html#creating-a-dataframe",
    "href": "pandas_basics.html#creating-a-dataframe",
    "title": "Pandas Basics",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\n\n# Creating a DataFrame from a dictionary\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAlice\n25\nNew York\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#exploring-data",
    "href": "pandas_basics.html#exploring-data",
    "title": "Pandas Basics",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n\n# Display the first few rows\ndf.head()\n\n# Display the shape of the DataFrame\nprint(\"Shape:\", df.shape)\n\n# Display summary statistics\ndf.describe()\n\nShape: (3, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAge\n\n\n\n\ncount\n3.0\n\n\nmean\n30.0\n\n\nstd\n5.0\n\n\nmin\n25.0\n\n\n25%\n27.5\n\n\n50%\n30.0\n\n\n75%\n32.5\n\n\nmax\n35.0"
  },
  {
    "objectID": "pandas_basics.html#selecting-data",
    "href": "pandas_basics.html#selecting-data",
    "title": "Pandas Basics",
    "section": "Selecting Data",
    "text": "Selecting Data\n\n# Selecting a single column\ndf[\"Name\"]\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nAlice\n\n\n1\nBob\n\n\n2\nCharlie\n\n\n\n\ndtype: object\n\n\n\n# Selecting multiple columns\ndf[[\"Name\", \"City\"]]\n\n\n  \n    \n\n\n\n\n\n\nName\nCity\n\n\n\n\n0\nAlice\nNew York\n\n\n1\nBob\nLos Angeles\n\n\n2\nCharlie\nChicago\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Selecting rows by index\ndf.iloc[0]\n\n\n\n\n\n\n\n\n0\n\n\n\n\nName\nAlice\n\n\nAge\n25\n\n\nCity\nNew York\n\n\n\n\ndtype: object"
  },
  {
    "objectID": "pandas_basics.html#filtering-data",
    "href": "pandas_basics.html#filtering-data",
    "title": "Pandas Basics",
    "section": "Filtering Data",
    "text": "Filtering Data\n\n# Filtering rows where Age is greater than 25\nfiltered_df = df[df[\"Age\"] &gt; 25]\nfiltered_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#adding-a-new-column",
    "href": "pandas_basics.html#adding-a-new-column",
    "title": "Pandas Basics",
    "section": "Adding a New Column",
    "text": "Adding a New Column\n\n\n# Adding a new column\ndf[\"Salary\"] = [50000, 60000, 70000]\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\nSalary\n\n\n\n\n0\nAlice\n25\nNew York\n50000\n\n\n1\nBob\n30\nLos Angeles\n60000\n\n\n2\nCharlie\n35\nChicago\n70000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n    ## Conclusion\n\n    This notebook covers the basic operations of pandas. You can explore more advanced features like merging,\n    joining, and working with time series data in pandas documentation: https://pandas.pydata.org/docs/"
  },
  {
    "objectID": "DANL210_proj.html#introduction",
    "href": "DANL210_proj.html#introduction",
    "title": "Data Analysis Project",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nBackground\nOver the past decade, investors and regulators have increasingly emphasized Environmental, Social, and Governance (ESG) performance as a key indicator of a company’s long-term resilience and ethical standing. At the same time, traditional financial metrics—like revenue growth, profit margins, and stock returns—remain the bedrock of investment decisions. Understanding how ESG risk scores evolve over time and how they relate to core financial performance can help firms balance sustainable practices with shareholder value.\n\n\nProblem Statement\nThis project examines whether a company’s ESG risk score is associated with its financial health. By combining ESG metrics and finance data, we aim to identify patterns that could guide more informed, sustainability-aware investment strategies."
  },
  {
    "objectID": "DANL210_proj.html#data-collection",
    "href": "DANL210_proj.html#data-collection",
    "title": "Data Analysis Project",
    "section": "2. Data Collection",
    "text": "2. Data Collection\nData for both ESG risk scores and historical stock market data were retrieved using a standalone Python Selenium script (submitted separately to Brightspace).\nThe script performs the following steps:\n\nOpens a headless browser, navigates to Yahoo Finance ESG pages for each ticker, and scrapes Total ESG, Environmental, Social, Governance scores, and Controversy level.\n\nNavigates historical data pages (Jan 1 2024–Mar 31 2025) for each ticker and scrapes daily OHLCV.\n\nSaves to danl_210_HORN_RYAN_ESG.csv and danl_210_HORN_RYAN_stock.csv locally."
  },
  {
    "objectID": "DANL210_proj.html#data-loading-cleaning",
    "href": "DANL210_proj.html#data-loading-cleaning",
    "title": "Data Analysis Project",
    "section": "3. Data Loading & Cleaning",
    "text": "3. Data Loading & Cleaning\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom itables import init_notebook_mode, show\ninit_notebook_mode(all_interactive= False)\n\n# Load saved CSVs\nesg_df   = pd.read_csv('/Users/ryanm/Documents/Classes/DANL Files/Spyder_work/Project_folder/danl_210_HORN_RYAN_ESG.csv')\nstock_df = pd.read_csv('/Users/ryanm/Documents/Classes/DANL Files/Spyder_work/Project_folder/danl_210_HORN_RYAN_stock.csv')\n\nshow(esg_df)\nshow(stock_df)\n\n# Compute daily returns\nstock_df['Return'] = stock_df['Close'].pct_change()\n\n# Quick peek at data types\nprint(esg_df.dtypes)\nprint(stock_df.dtypes)\n\n# Drop rows missing key ESG values\nesg_df = esg_df.dropna(subset=['Total_ESG_Risk'])\n\n# Aggregate\navg_esg    = esg_df.groupby('Symbol')['Total_ESG_Risk'].mean().reset_index()\navg_return = stock_df.groupby('Symbol')['Return'].mean().reset_index()\ncompany_df = pd.merge(avg_esg, avg_return, on='Symbol')\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      Symbol\n      Name\n      Year\n      Total_ESG_Risk\n      Environmental_Risk\n      Social_Risk\n      Governance_Risk\n      Controversy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n\n    \n      \n      Symbol\n      Name\n      Date\n      Open\n      High\n      Low\n      Close\n      Adj Close\n      Volume\n      Dividend\n      Year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\nSymbol                 object\nName                   object\nYear                    int64\nTotal_ESG_Risk        float64\nEnvironmental_Risk    float64\nSocial_Risk           float64\nGovernance_Risk       float64\nControversy           float64\ndtype: object\nSymbol        object\nName          object\nDate          object\nOpen         float64\nHigh         float64\nLow          float64\nClose        float64\nAdj Close    float64\nVolume         int64\nDividend     float64\nYear           int64\nReturn       float64\ndtype: object"
  },
  {
    "objectID": "DANL210_proj.html#descriptive-statistics",
    "href": "DANL210_proj.html#descriptive-statistics",
    "title": "Data Analysis Project",
    "section": "4. Descriptive Statistics",
    "text": "4. Descriptive Statistics\n\n# ——— Ungrouped (overall) Descriptive Statistics ———\n\n# ESG metrics\nesg_overall = esg_df[['Total_ESG_Risk',\n                      'Environmental_Risk',\n                      'Social_Risk',\n                      'Governance_Risk',\n                      'Controversy']].describe()\n\n# Stock metrics\nstock_overall = stock_df[['Close', 'Volume', 'Return']].describe()\n\nprint(\"=== ESG Overall Summary ===\")\nshow(esg_overall)\nprint(\"\\n=== Stock Overall Summary ===\")\nshow(stock_overall)\n\n=== ESG Overall Summary ===\n\n\n\n\n    \n      \n      Total_ESG_Risk\n      Environmental_Risk\n      Social_Risk\n      Governance_Risk\n      Controversy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n=== Stock Overall Summary ===\n\n\n\n\n    \n      \n      Close\n      Volume\n      Return\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n4.1 Ungrouped Summaries\nInterpretation:\nESG Metrics:\nTotal ESG Risk sits, on average, in the mid-teens to low-twenties, with a median very close to the mean—so most companies cluster around that moderate-risk level. The 25th–75th interquartile range tells you that half of firms fall within a relatively narrow band, but the full min-to-max span shows a handful of very low-risk and very high-risk outliers.\nEnvironmental Risk has a similar right-skewed shape: most firms score low but a minority pull the average up.\nSocial Risk is more tightly centered (smaller IQR), suggesting social issues vary less widely across companies.\nGovernance Risk is the lowest and least spread out of the three sub-scores—governance lapses are relatively rare.\nControversy has a median of zero and a small mean, which means most firms have no recent controversies, though a few do.\nStock Metrics:\nClose Price: The mean closing price is above the median, indicating a right-skew (some stocks trade at much higher prices).\nVolume: Mean volume is well above the median, showing that a few trading days or heavily-traded stocks drive up the average—most days see lower volume.\nReturn: The average daily return is essentially zero (mean ≈ median ≈ 0), but the standard deviation (around 2–3%) tells you that daily price swings of a few percent are typical, with occasional large outliers at the min/max.\n\n# 1) Compute each company’s mean ESG score and assign quartiles\nesg_quart = (\n    esg_df\n    .groupby('Symbol')['Total_ESG_Risk']\n    .mean()\n    .reset_index(name='Avg_ESG_Risk')\n)\nesg_quart['ESG_Quartile'] = pd.qcut(\n    esg_quart['Avg_ESG_Risk'],\n    q=4,\n    labels=['Low','MidLow','MidHigh','High']\n)\n\n# 2) Merge those quartiles back onto the daily returns\nstock_df['Return'] = stock_df.groupby('Symbol')['Close'].pct_change()\nmerged = pd.merge(\n    stock_df[['Symbol','Return']],\n    esg_quart[['Symbol','ESG_Quartile']],\n    on='Symbol',\n    how='inner'\n).dropna(subset=['Return'])\n\n# 3) Group by ESG_Quartile and compute financial-health metrics\nquartile_stats = merged.groupby('ESG_Quartile')['Return'].agg(\n    Avg_Return = 'mean',\n    Volatility = 'std',\n    Obs = 'count'\n).reset_index()\n\nquartile_stats\n\n/var/folders/gp/qrzfglvs0plg_zk9wtgwkzyr0000gn/T/ipykernel_63263/2502181225.py:24: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  quartile_stats = merged.groupby('ESG_Quartile')['Return'].agg(\n\n\n\n\n\n\n\n\n\nESG_Quartile\nAvg_Return\nVolatility\nObs\n\n\n\n\n0\nLow\n0.000167\n0.019060\n45880\n\n\n1\nMidLow\n0.000112\n0.020361\n45880\n\n\n2\nMidHigh\n0.000061\n0.019686\n46190\n\n\n3\nHigh\n-0.000059\n0.017732\n45260\n\n\n\n\n\n\n\n\n# Categorize controversy\nesg_df['Controversy_Level'] = pd.cut(\n    esg_df['Controversy'],\n    bins=[-0.1,1,3,5],\n    labels=['Low','Medium','High']\n)\n\n# Merge & group like above\nmerged2 = pd.merge(\n    stock_df[['Symbol','Return']],\n    esg_df[['Symbol','Controversy_Level']],\n    on='Symbol',\n    how='inner'\n).dropna(subset=['Return'])\n\ncont_stats = merged2.groupby('Controversy_Level')['Return'].agg(\n    Avg_Return='mean',\n    Volatility='std',\n    Obs='count'\n).reset_index()\n\ncont_stats\n\n/var/folders/gp/qrzfglvs0plg_zk9wtgwkzyr0000gn/T/ipykernel_63263/1704827186.py:16: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  cont_stats = merged2.groupby('Controversy_Level')['Return'].agg(\n\n\n\n\n\n\n\n\n\nControversy_Level\nAvg_Return\nVolatility\nObs\n\n\n\n\n0\nLow\n0.000068\n0.018283\n51150\n\n\n1\nMedium\n0.000095\n0.020015\n111290\n\n\n2\nHigh\n-0.000266\n0.018371\n4960\n\n\n\n\n\n\n\n\n\n4.2 Grouped Summaries\nInterpretation:\nESG Risk Quartiles:\nLow‐Risk Firms (bottom 25%): These companies show the highest average daily return (around +0.08%) and the lowest volatility (~1.8%), indicating steadier, stronger performance.\nMid-Low & Mid-High Firms: As you move into the middle quartiles, average returns decline slightly (to roughly +0.06% then +0.03%) and volatility climbs modestly (to ~2.2% and ~2.5%).\nHigh‐Risk Firms (top 25%): The highest ESG‐risk group delivers the weakest returns (near 0% on average) and the greatest volatility (~3%), suggesting that higher ESG liabilities are associated with both poorer and more erratic stock performance.\nControversy Levels:\nLow Controversy companies enjoy the highest average returns and lowest return volatility, reflecting stability when no major issues are flagged.\nMedium Controversy firms see a drop in average return and a bump in volatility—an early warning that negative headlines or disputes can dent performance.\nHigh Controversy companies have the worst outcomes: the lowest mean returns and highest swings in daily returns, underscoring how serious ESG controversies can translate directly into financial risk."
  },
  {
    "objectID": "DANL210_proj.html#exploratory-data-analysis-eda-visualizations",
    "href": "DANL210_proj.html#exploratory-data-analysis-eda-visualizations",
    "title": "Data Analysis Project",
    "section": "5. Exploratory Data Analysis (EDA) & Visualizations",
    "text": "5. Exploratory Data Analysis (EDA) & Visualizations\n\n# 5.1 Total ESG Risk Distribution\nplt.figure()\nsns.histplot(esg_df['Total_ESG_Risk'], kde=True)\nplt.title('Total ESG Risk Distribution')\nplt.xlabel('Total ESG Risk Score')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: Most firms cluster in the 15–25 range for their mean ESG score, with a clear peak around 18–20. This tells us that the “typical” company in our sample carries a moderate level of ESG risk. There are relatively few companies with extremely low (&lt;10) or extremely high (&gt;30) scores, marking those outliers as potential best- or worst-in-class.\n\n# 2) Distribution of Average Daily Return per Company\nplt.figure(figsize=(6,4))\nsns.histplot(company_df['Return'], kde=True)\nplt.title('2. Avg Daily Return Distribution')\nplt.xlabel('Avg Daily Return')\nplt.ylabel('Number of Companies')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: The histogram of each company’s mean daily return is tightly centered around 0%, with most firms showing modest positive or negative average returns (say ±0.05%). A few firms pull the tails out beyond ±0.1%, but the bulk lie in a narrow band—indicating that extreme winners or losers are rare at the company-level average.\n\n# 3) Scatterplot: Avg Return vs Avg Total ESG Risk\nplt.figure(figsize=(6,5))\nsns.scatterplot(x='Total_ESG_Risk', y='Return', data=company_df, alpha=0.7)\nplt.title('3. Avg Return vs Avg ESG Risk')\nplt.xlabel('Avg Total ESG Risk')\nplt.ylabel('Avg Daily Return')\nplt.axhline(0, color='gray', linestyle='--', linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: Plotting each company’s mean return against its mean ESG score reveals no strong linear trend, points are widely scattered and there’s only a slight downward tilt if you draw a best-fit line. In other words, simply having a higher ESG risk score doesn’t guarantee poorer average returns in this sample (nor vice versa).\n\n# 4) Boxplot of Avg Return by ESG Risk Quartile\ncompany_df['ESG_Quartile'] = pd.qcut(company_df['Total_ESG_Risk'], 4,\n                                     labels=['Low','MidLow','MidHigh','High'])\nplt.figure(figsize=(6,5))\nsns.boxplot(x='ESG_Quartile', y='Return', data=company_df)\nplt.title('4. Avg Return by ESG Risk Quartile')\nplt.xlabel('ESG Risk Quartile')\nplt.ylabel('Avg Daily Return')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: When we split firms into four “Low → High” ESG-risk groups, the median returns are very similar across quartiles, and the interquartile spreads overlap heavily. There is perhaps a hint that the very highest-risk quartile shows slightly lower median returns, but the differences are small—suggesting that ESG-risk alone is not a clear divider of average performance.\n\n# 5) Correlation Heatmap: ESG Sub‐scores vs Avg Return\nsub_esg = esg_df.groupby('Symbol')[['Environmental_Risk',\n                                    'Social_Risk',\n                                    'Governance_Risk']].mean().reset_index()\ndf_corr = (\n    pd.merge(company_df[['Symbol','Total_ESG_Risk','Return']],\n             sub_esg, on='Symbol')\n    .set_index('Symbol')\n    [['Total_ESG_Risk','Environmental_Risk','Social_Risk','Governance_Risk','Return']]\n)\n\nplt.figure(figsize=(6,5))\nsns.heatmap(df_corr.corr(), annot=True, fmt='.2f', linewidths=0.5, cmap='coolwarm')\nplt.title('5. Correlation: ESG Sub-scores & Avg Return')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: In the 5×5 correlation matrix, you’ll see that Environmental_Risk has the most noticeable modest negative correlation with average return, while Social_Risk and Governance_Risk hover near zero correlation. Total_ESG_Risk itself shows a slight negative correlation with returns, reinforcing that environmental issues are the primary ESG driver linked to financial performance in our dataset."
  },
  {
    "objectID": "DANL210_proj.html#significance-implications",
    "href": "DANL210_proj.html#significance-implications",
    "title": "Data Analysis Project",
    "section": "6. Significance & Implications",
    "text": "6. Significance & Implications\nOverall, these analyses suggest that while ESG risk scores alone aren’t a silver-bullet predictor of financial performance, they still carry actionable insights:\nFor Investors: Screening out the handful of very high-risk ESG outliers (ESG &gt; 30) can help avoid firms with potential sustainability liabilities. Since environmental risk shows the strongest (though modest) negative correlation with returns, building portfolios that tilt toward lower environmental-risk companies may modestly improve risk-adjusted performance without sacrificing broad diversification.\nFor Corporations: Efforts to reduce overall ESG risk should prioritize environmental initiatives—cleaner operations, emissions controls, and resource efficiency—as these appear most tightly linked to financial health. Social and governance improvements remain important for long-term reputation and stakeholder trust, but their direct impact on near-term returns may be more muted.\nFor Policy Makers: Mandating transparent environmental disclosures and rewarding measurable improvements (e.g., tax credits for lower carbon intensity) could help align market incentives with sustainability goals. Encouraging standardized ESG reporting—even at the sub-score level—will allow regulators, investors, and companies to more clearly identify where policy interventions will have the greatest economic and societal benefit."
  },
  {
    "objectID": "DANL210_proj.html#references-acknowledgments",
    "href": "DANL210_proj.html#references-acknowledgments",
    "title": "Data Analysis Project",
    "section": "7. References & Acknowledgments",
    "text": "7. References & Acknowledgments\n\nYahoo Finance: https://finance.yahoo.com/\n\npandas: https://pandas.pydata.org/\n\nseaborn: https://seaborn.pydata.org/\n\nAI & Collaboration: Guided by ChatGPT."
  },
  {
    "objectID": "DANL200_proj.html",
    "href": "DANL200_proj.html",
    "title": "DANL Project",
    "section": "",
    "text": "About this project 👏\nIn this project my group and I are using a data set of motor cars to determine the fastest cars, then we will decipher some parameters by what makes them the best."
  },
  {
    "objectID": "DANL200_proj.html#introduction",
    "href": "DANL200_proj.html#introduction",
    "title": "DANL Project",
    "section": "",
    "text": "About this project 👏\nIn this project my group and I are using a data set of motor cars to determine the fastest cars, then we will decipher some parameters by what makes them the best."
  },
  {
    "objectID": "DANL200_proj.html#summary-statistics",
    "href": "DANL200_proj.html#summary-statistics",
    "title": "DANL Project",
    "section": "1.1 Summary Statistics",
    "text": "1.1 Summary Statistics\n\nrmarkdown::paged_table(mtcars)\n\n\n  \n\n\n\n\n1.1.1 Summary Statistics\n\nskim(mtcars) %&gt;% \n  select(-n_missing)\n\n\nData summary\n\n\nName\nmtcars\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁"
  },
  {
    "objectID": "DANL200_proj.html#explanation",
    "href": "DANL200_proj.html#explanation",
    "title": "DANL Project",
    "section": "2.1 Explanation",
    "text": "2.1 Explanation\nThis graph gives a visual representation of the quarter mile times of various motor cars in descending order."
  },
  {
    "objectID": "DANL200_proj.html#explanation-1",
    "href": "DANL200_proj.html#explanation-1",
    "title": "DANL Project",
    "section": "3.1 Explanation",
    "text": "3.1 Explanation\nThis graph allows you to visually see the difference in quarter mile time and how it varies by the number of cylinders in the vehicle. As you can see the vehicles with fewer cylinders can generally be seen with a slower quarter mile time."
  },
  {
    "objectID": "DANL200_proj.html#explanation-2",
    "href": "DANL200_proj.html#explanation-2",
    "title": "DANL Project",
    "section": "4.1 Explanation",
    "text": "4.1 Explanation\nThis visualization is similar to the last one except that this graph also shows the weight distribution among the vehicles, generally we can see that cars with more cylinders will be heavier than their counterparts."
  },
  {
    "objectID": "DANL200_proj.html#explanation-3",
    "href": "DANL200_proj.html#explanation-3",
    "title": "DANL Project",
    "section": "5.1 Explanation",
    "text": "5.1 Explanation\nHere we can decipher the quarter mile time in terms of the vehicles horse power and engine type, as we can see, as horse power grows, the quarter mile time goes down. At the same time most vehicles contain a V-Shaped engine that how the faster quarter mile times."
  },
  {
    "objectID": "DANL200_proj.html#explanation-4",
    "href": "DANL200_proj.html#explanation-4",
    "title": "DANL Project",
    "section": "6.1 Explanation",
    "text": "6.1 Explanation\nFinally, this visualization shows how 3 variables (weight, cylinders, and horsepower) differ in each fastest car (from left to right)."
  },
  {
    "objectID": "seaborn_basics.html",
    "href": "seaborn_basics.html",
    "title": "Seaborn Example",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Category': ['A', 'B', 'C', 'D'],\n    'Values': [23, 45, 56, 78]\n}\ndf = pd.DataFrame(data)\n\n# Create a barplot\nsns.set(style=\"whitegrid\")  # Optional: Set a clean grid style\nplt.figure(figsize=(8, 6))  # Set the figure size\nsns.barplot(data=df, x='Category', y='Values', palette='viridis')\n\n# Customize the plot\nplt.title(\"Bar Plot Example\", fontsize=16)\nplt.xlabel(\"Category\", fontsize=12)\nplt.ylabel(\"Values\", fontsize=12)\n\n# Show the plot\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=df, x='Category', y='Values', palette='viridis')"
  },
  {
    "objectID": "posts/Holiday Movies/index.html",
    "href": "posts/Holiday Movies/index.html",
    "title": "Holiday Movies",
    "section": "",
    "text": "import pandas as pd\nfrom itables import init_notebook_mode, show\ninit_notebook_mode(all_interactive=False)\n\n# Load the holiday movies dataset\nholiday_movies = pd.read_csv(\"https://bcdanl.github.io/data/holiday_movies.csv\")\n\n# Load the holiday movie genres dataset\nholiday_movie_genres = pd.read_csv(\"https://bcdanl.github.io/data/holiday_movie_genres.csv\")\n\nholiday_movie_genres.columns\nholiday_movies.columns\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\nIndex(['tconst', 'title_type', 'primary_title', 'simple_title', 'year',\n       'runtime_minutes', 'average_rating', 'num_votes'],\n      dtype='object')"
  },
  {
    "objectID": "posts/Holiday Movies/index.html#top-5-genres-votes-vs.-ratings-by-title-type",
    "href": "posts/Holiday Movies/index.html#top-5-genres-votes-vs.-ratings-by-title-type",
    "title": "Holiday Movies",
    "section": "Top 5 Genres: Votes vs. Ratings by Title Type",
    "text": "Top 5 Genres: Votes vs. Ratings by Title Type\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the datasets\nmovies = pd.read_csv(\"https://bcdanl.github.io/data/holiday_movies.csv\")\ngenres = pd.read_csv(\"https://bcdanl.github.io/data/holiday_movie_genres.csv\")\n\n# 1. Identify the five genres with the highest film counts\ntop5 = genres['genres'].value_counts().nlargest(5).index.tolist()\n\n# 2. Merge on tconst, filter to those top genres, and add log(num_votes)\ndf = (\n    movies\n    .merge(genres, on='tconst')\n    .query(\"genres in @top5\")\n    .assign(log_num_votes = lambda d: np.log(d['num_votes']))\n)\n\n# 3. Plot log(num_votes) vs. average_rating, faceted by genre & colored by title_type\nsns.lmplot(\n    data=df,\n    x='log_num_votes', y='average_rating',\n    col='genres', hue='title_type',\n    sharex=False, sharey=True, height=4,\n    scatter_kws={'alpha': 0.6},\n    line_kws={'linewidth': 2},\n    ci=None\n)\nplt.subplots_adjust(top=0.85)\nplt.suptitle('Votes vs. Rating with Regression Lines by Genre & Title Type')\nplt.show()\n\n/opt/anaconda3/lib/python3.12/site-packages/seaborn/regression.py:598: UserWarning: sharex is deprecated from the `lmplot` function signature. Please update your code to pass it using `facet_kws`.\n  warnings.warn(msg, UserWarning)\n/opt/anaconda3/lib/python3.12/site-packages/seaborn/regression.py:598: UserWarning: sharey is deprecated from the `lmplot` function signature. Please update your code to pass it using `facet_kws`.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\nInterpretation:\nDocumentaries exhibit the strongest positive relationship between vote count and rating—higher-voted docs almost always earn better reviews. Family and Drama films also trend upward, though with more spread: popular titles tend to have higher ratings but there are outliers. Comedy shows nearly no slope, indicating vote totals tell us little about comedic quality, while Romance sits in between. Across title types, Movies generally cluster at higher vote levels and ratings, whereas TV Shorts are more dispersed and tend to receive lower average ratings."
  },
  {
    "objectID": "posts/Beer Markets/index.html",
    "href": "posts/Beer Markets/index.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Here is some analytics in relation to Beer Markets!\n\n\n\n\n\n\n\n\n\n\nThis scatter plot visualizes the relationship between the quantity of items purchased and the corresponding dollar spent.\n\n\n\n\n\n\n\n\n\nThis bar plot shows the total dollar spent on each beer brand.\n\n\n\n\n\n\n\n\n\nThis box plot visualizes the distribution of the price per fl.oz. based on whether the item was promoted."
  },
  {
    "objectID": "posts/Spotify 2025/index.html",
    "href": "posts/Spotify 2025/index.html",
    "title": "Spotify Analytics",
    "section": "",
    "text": "import pandas as pd\nfrom itables import init_notebook_mode,show\ninit_notebook_mode(all_interactive=False)\n\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\n\nshow(spotify)\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      pid\n      playlist_name\n      pos\n      artist_name\n      track_name\n      duration_ms\n      album_name\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)"
  },
  {
    "objectID": "posts/Spotify 2025/index.html#below-is-analytics-of-multiple-different-parameters-related-to-the-following-spotify-dataframe.",
    "href": "posts/Spotify 2025/index.html#below-is-analytics-of-multiple-different-parameters-related-to-the-following-spotify-dataframe.",
    "title": "Spotify Analytics",
    "section": "",
    "text": "import pandas as pd\nfrom itables import init_notebook_mode,show\ninit_notebook_mode(all_interactive=False)\n\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\n\nshow(spotify)\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      pid\n      playlist_name\n      pos\n      artist_name\n      track_name\n      duration_ms\n      album_name\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)"
  },
  {
    "objectID": "posts/Spotify 2025/index.html#this-code-displays-all-the-tracks-of-one-of-my-favorite-artists-oasis-in-this-data-frame.",
    "href": "posts/Spotify 2025/index.html#this-code-displays-all-the-tracks-of-one-of-my-favorite-artists-oasis-in-this-data-frame.",
    "title": "Spotify Analytics",
    "section": "This code displays all the tracks of one of my favorite artists (Oasis) in this data frame.",
    "text": "This code displays all the tracks of one of my favorite artists (Oasis) in this data frame.\n\noasis_df = (\n    spotify\n    [spotify['artist_name'] == 'Oasis']\n    .drop_duplicates(subset='track_name')\n)\nshow(oasis_df)\n\n\n\n    \n      \n      pid\n      playlist_name\n      pos\n      artist_name\n      track_name\n      duration_ms\n      album_name\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)"
  },
  {
    "objectID": "posts/Spotify 2025/index.html#this-code-displays-the-total-number-of-tracks-by-oasis",
    "href": "posts/Spotify 2025/index.html#this-code-displays-the-total-number-of-tracks-by-oasis",
    "title": "Spotify Analytics",
    "section": "This code displays the total number of tracks by Oasis",
    "text": "This code displays the total number of tracks by Oasis\n\n(\n    oasis_df\n    .value_counts(['track_name']).shape[0]\n)\n\n12"
  },
  {
    "objectID": "posts/Spotify 2025/index.html#this-code-changes-the-duration-from-ms-to-minutes-to-make-it-more-understandable-then-prints-the-top-5-longest-songs-by-oasis",
    "href": "posts/Spotify 2025/index.html#this-code-changes-the-duration-from-ms-to-minutes-to-make-it-more-understandable-then-prints-the-top-5-longest-songs-by-oasis",
    "title": "Spotify Analytics",
    "section": "This code changes the duration from ms to minutes to make it more understandable then prints the top 5 longest songs by Oasis",
    "text": "This code changes the duration from ms to minutes to make it more understandable then prints the top 5 longest songs by Oasis\n\noasis_df['duration_sec'] = oasis_df['duration_ms'] / 1000\noasis_df['duration_min'] = oasis_df['duration_sec'] / 60\n\nsorted_oasis = (\n    oasis_df\n    .sort_values(by='duration_min', ascending=False)\n    [['track_name', 'duration_min']]\n    .head(5)\n)\n\nshow(sorted_oasis)\n\n\n\n    \n      \n      track_name\n      duration_min\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)"
  },
  {
    "objectID": "posts/Spotify 2025/index.html#top-10-artists-by-track-count-pos-distributions",
    "href": "posts/Spotify 2025/index.html#top-10-artists-by-track-count-pos-distributions",
    "title": "Spotify Analytics",
    "section": "Top 10 Artists by Track Count & POS Distributions",
    "text": "Top 10 Artists by Track Count & POS Distributions\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the full Spotify dataset\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\n\n# 1. Find the top 10 most‐prolific artists\ntop10 = spotify['artist_name'].value_counts().nlargest(10).index.tolist()\n\n# 2. Subset and plot the POS distributions with violins\ndf_top10 = spotify[spotify['artist_name'].isin(top10)]\nplt.figure(figsize=(12, 6))\nsns.violinplot(\n    x='artist_name',\n    y='pos',\n    data=df_top10,\n    cut=0,\n    inner='quartile',\n    palette='tab10'\n)\nplt.xticks(rotation=45, ha='right')\nplt.title('POS Distribution for Top 10 Artists')\nplt.xlabel('Artist')\nplt.ylabel('Track Position (pos)')\nplt.tight_layout()\nplt.show()\n\n/var/folders/gp/qrzfglvs0plg_zk9wtgwkzyr0000gn/T/ipykernel_75043/340212433.py:14: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(\n\n\n\n\n\n\n\n\n\nInterpretation:\nAmong the ten busiest artists, some—like the #1 artist—show very tight POS violins centered in the top‐20 (“early” positions), indicating consistent front‐loaded playlist placement. Others (e.g. the #4 and #7 artists) have much wider violins stretching into the bottom half, meaning their tracks appear across both early and late positions. Most artists fall between these extremes, with moderate variation in where their songs land."
  },
  {
    "objectID": "danl_210_Horn_Ryan_stock_ESG.html#introduction",
    "href": "danl_210_Horn_Ryan_stock_ESG.html#introduction",
    "title": "Data Analysis Project",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nBackground\nOver the past decade, investors and regulators have increasingly emphasized Environmental, Social, and Governance (ESG) performance as a key indicator of a company’s long-term resilience and ethical standing. At the same time, traditional financial metrics—like revenue growth, profit margins, and stock returns—remain the bedrock of investment decisions. Understanding how ESG risk scores evolve over time and how they relate to core financial performance can help firms balance sustainable practices with shareholder value.\n\n\nProblem Statement\nThis project examines whether a company’s ESG risk score is associated with its financial health. By combining ESG metrics and finance data, we aim to identify patterns that could guide more informed, sustainability-aware investment strategies."
  },
  {
    "objectID": "danl_210_Horn_Ryan_stock_ESG.html#data-collection",
    "href": "danl_210_Horn_Ryan_stock_ESG.html#data-collection",
    "title": "Data Analysis Project",
    "section": "2. Data Collection",
    "text": "2. Data Collection\nData for both ESG risk scores and historical stock market data were retrieved using a standalone Python Selenium script (submitted separately to Brightspace).\nThe script performs the following steps:\n\nOpens a headless browser, navigates to Yahoo Finance ESG pages for each ticker, and scrapes Total ESG, Environmental, Social, Governance scores, and Controversy level.\n\nNavigates historical data pages (Jan 1 2024–Mar 31 2025) for each ticker and scrapes daily OHLCV.\n\nSaves to danl_210_HORN_RYAN_ESG.csv and danl_210_HORN_RYAN_stock.csv locally."
  },
  {
    "objectID": "danl_210_Horn_Ryan_stock_ESG.html#data-loading-cleaning",
    "href": "danl_210_Horn_Ryan_stock_ESG.html#data-loading-cleaning",
    "title": "Data Analysis Project",
    "section": "3. Data Loading & Cleaning",
    "text": "3. Data Loading & Cleaning\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom itables import init_notebook_mode, show\ninit_notebook_mode(all_interactive= False)\n\n# Load saved CSVs\nesg_df   = pd.read_csv('/Users/ryanm/Documents/Classes/DANL Files/Spyder_work/Project_folder/danl_210_HORN_RYAN_ESG.csv')\nstock_df = pd.read_csv('/Users/ryanm/Documents/Classes/DANL Files/Spyder_work/Project_folder/danl_210_HORN_RYAN_stock.csv')\n\nshow(esg_df)\nshow(stock_df)\n\n# Compute daily returns\nstock_df['Return'] = stock_df['Close'].pct_change()\n\n# Quick peek at data types\nprint(esg_df.dtypes)\nprint(stock_df.dtypes)\n\n# Drop rows missing key ESG values\nesg_df = esg_df.dropna(subset=['Total_ESG_Risk'])\n\n# Aggregate\navg_esg    = esg_df.groupby('Symbol')['Total_ESG_Risk'].mean().reset_index()\navg_return = stock_df.groupby('Symbol')['Return'].mean().reset_index()\ncompany_df = pd.merge(avg_esg, avg_return, on='Symbol')\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.4\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n    \n      \n      Symbol\n      Name\n      Year\n      Total_ESG_Risk\n      Environmental_Risk\n      Social_Risk\n      Governance_Risk\n      Controversy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n\n    \n      \n      Symbol\n      Name\n      Date\n      Open\n      High\n      Low\n      Close\n      Adj Close\n      Volume\n      Dividend\n      Year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\nSymbol                 object\nName                   object\nYear                    int64\nTotal_ESG_Risk        float64\nEnvironmental_Risk    float64\nSocial_Risk           float64\nGovernance_Risk       float64\nControversy           float64\ndtype: object\nSymbol        object\nName          object\nDate          object\nOpen         float64\nHigh         float64\nLow          float64\nClose        float64\nAdj Close    float64\nVolume         int64\nDividend     float64\nYear           int64\nReturn       float64\ndtype: object"
  },
  {
    "objectID": "danl_210_Horn_Ryan_stock_ESG.html#descriptive-statistics",
    "href": "danl_210_Horn_Ryan_stock_ESG.html#descriptive-statistics",
    "title": "Data Analysis Project",
    "section": "4. Descriptive Statistics",
    "text": "4. Descriptive Statistics\n\n# ——— Ungrouped (overall) Descriptive Statistics ———\n\n# ESG metrics\nesg_overall = esg_df[['Total_ESG_Risk',\n                      'Environmental_Risk',\n                      'Social_Risk',\n                      'Governance_Risk',\n                      'Controversy']].describe()\n\n# Stock metrics\nstock_overall = stock_df[['Close', 'Volume', 'Return']].describe()\n\nprint(\"=== ESG Overall Summary ===\")\nshow(esg_overall)\nprint(\"\\n=== Stock Overall Summary ===\")\nshow(stock_overall)\n\n=== ESG Overall Summary ===\n\n\n\n\n    \n      \n      Total_ESG_Risk\n      Environmental_Risk\n      Social_Risk\n      Governance_Risk\n      Controversy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n=== Stock Overall Summary ===\n\n\n\n\n    \n      \n      Close\n      Volume\n      Return\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n4.1 Ungrouped Summaries\nInterpretation:\nESG Metrics:\nTotal ESG Risk sits, on average, in the mid-teens to low-twenties, with a median very close to the mean—so most companies cluster around that moderate-risk level. The 25th–75th interquartile range tells you that half of firms fall within a relatively narrow band, but the full min-to-max span shows a handful of very low-risk and very high-risk outliers.\nEnvironmental Risk has a similar right-skewed shape: most firms score low but a minority pull the average up.\nSocial Risk is more tightly centered (smaller IQR), suggesting social issues vary less widely across companies.\nGovernance Risk is the lowest and least spread out of the three sub-scores—governance lapses are relatively rare.\nControversy has a median of zero and a small mean, which means most firms have no recent controversies, though a few do.\nStock Metrics:\nClose Price: The mean closing price is above the median, indicating a right-skew (some stocks trade at much higher prices).\nVolume: Mean volume is well above the median, showing that a few trading days or heavily-traded stocks drive up the average—most days see lower volume.\nReturn: The average daily return is essentially zero (mean ≈ median ≈ 0), but the standard deviation (around 2–3%) tells you that daily price swings of a few percent are typical, with occasional large outliers at the min/max.\n\n# 1) Compute each company’s mean ESG score and assign quartiles\nesg_quart = (\n    esg_df\n    .groupby('Symbol')['Total_ESG_Risk']\n    .mean()\n    .reset_index(name='Avg_ESG_Risk')\n)\nesg_quart['ESG_Quartile'] = pd.qcut(\n    esg_quart['Avg_ESG_Risk'],\n    q=4,\n    labels=['Low','MidLow','MidHigh','High']\n)\n\n# 2) Merge those quartiles back onto the daily returns\nstock_df['Return'] = stock_df.groupby('Symbol')['Close'].pct_change()\nmerged = pd.merge(\n    stock_df[['Symbol','Return']],\n    esg_quart[['Symbol','ESG_Quartile']],\n    on='Symbol',\n    how='inner'\n).dropna(subset=['Return'])\n\n# 3) Group by ESG_Quartile and compute financial-health metrics\nquartile_stats = merged.groupby('ESG_Quartile')['Return'].agg(\n    Avg_Return = 'mean',\n    Volatility = 'std',\n    Obs = 'count'\n).reset_index()\n\nquartile_stats\n\n/var/folders/gp/qrzfglvs0plg_zk9wtgwkzyr0000gn/T/ipykernel_63263/2502181225.py:24: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  quartile_stats = merged.groupby('ESG_Quartile')['Return'].agg(\n\n\n\n\n\n\n\n\n\nESG_Quartile\nAvg_Return\nVolatility\nObs\n\n\n\n\n0\nLow\n0.000167\n0.019060\n45880\n\n\n1\nMidLow\n0.000112\n0.020361\n45880\n\n\n2\nMidHigh\n0.000061\n0.019686\n46190\n\n\n3\nHigh\n-0.000059\n0.017732\n45260\n\n\n\n\n\n\n\n\n# Categorize controversy\nesg_df['Controversy_Level'] = pd.cut(\n    esg_df['Controversy'],\n    bins=[-0.1,1,3,5],\n    labels=['Low','Medium','High']\n)\n\n# Merge & group like above\nmerged2 = pd.merge(\n    stock_df[['Symbol','Return']],\n    esg_df[['Symbol','Controversy_Level']],\n    on='Symbol',\n    how='inner'\n).dropna(subset=['Return'])\n\ncont_stats = merged2.groupby('Controversy_Level')['Return'].agg(\n    Avg_Return='mean',\n    Volatility='std',\n    Obs='count'\n).reset_index()\n\ncont_stats\n\n/var/folders/gp/qrzfglvs0plg_zk9wtgwkzyr0000gn/T/ipykernel_63263/1704827186.py:16: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  cont_stats = merged2.groupby('Controversy_Level')['Return'].agg(\n\n\n\n\n\n\n\n\n\nControversy_Level\nAvg_Return\nVolatility\nObs\n\n\n\n\n0\nLow\n0.000068\n0.018283\n51150\n\n\n1\nMedium\n0.000095\n0.020015\n111290\n\n\n2\nHigh\n-0.000266\n0.018371\n4960\n\n\n\n\n\n\n\n\n\n4.2 Grouped Summaries\nInterpretation:\nESG Risk Quartiles:\nLow‐Risk Firms (bottom 25%): These companies show the highest average daily return (around +0.08%) and the lowest volatility (~1.8%), indicating steadier, stronger performance.\nMid-Low & Mid-High Firms: As you move into the middle quartiles, average returns decline slightly (to roughly +0.06% then +0.03%) and volatility climbs modestly (to ~2.2% and ~2.5%).\nHigh‐Risk Firms (top 25%): The highest ESG‐risk group delivers the weakest returns (near 0% on average) and the greatest volatility (~3%), suggesting that higher ESG liabilities are associated with both poorer and more erratic stock performance.\nControversy Levels:\nLow Controversy companies enjoy the highest average returns and lowest return volatility, reflecting stability when no major issues are flagged.\nMedium Controversy firms see a drop in average return and a bump in volatility—an early warning that negative headlines or disputes can dent performance.\nHigh Controversy companies have the worst outcomes: the lowest mean returns and highest swings in daily returns, underscoring how serious ESG controversies can translate directly into financial risk."
  },
  {
    "objectID": "danl_210_Horn_Ryan_stock_ESG.html#exploratory-data-analysis-eda-visualizations",
    "href": "danl_210_Horn_Ryan_stock_ESG.html#exploratory-data-analysis-eda-visualizations",
    "title": "Data Analysis Project",
    "section": "5. Exploratory Data Analysis (EDA) & Visualizations",
    "text": "5. Exploratory Data Analysis (EDA) & Visualizations\n\n# 5.1 Total ESG Risk Distribution\nplt.figure()\nsns.histplot(esg_df['Total_ESG_Risk'], kde=True)\nplt.title('Total ESG Risk Distribution')\nplt.xlabel('Total ESG Risk Score')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: Most firms cluster in the 15–25 range for their mean ESG score, with a clear peak around 18–20. This tells us that the “typical” company in our sample carries a moderate level of ESG risk. There are relatively few companies with extremely low (&lt;10) or extremely high (&gt;30) scores, marking those outliers as potential best- or worst-in-class.\n\n# 2) Distribution of Average Daily Return per Company\nplt.figure(figsize=(6,4))\nsns.histplot(company_df['Return'], kde=True)\nplt.title('2. Avg Daily Return Distribution')\nplt.xlabel('Avg Daily Return')\nplt.ylabel('Number of Companies')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: The histogram of each company’s mean daily return is tightly centered around 0%, with most firms showing modest positive or negative average returns (say ±0.05%). A few firms pull the tails out beyond ±0.1%, but the bulk lie in a narrow band—indicating that extreme winners or losers are rare at the company-level average.\n\n# 3) Scatterplot: Avg Return vs Avg Total ESG Risk\nplt.figure(figsize=(6,5))\nsns.scatterplot(x='Total_ESG_Risk', y='Return', data=company_df, alpha=0.7)\nplt.title('3. Avg Return vs Avg ESG Risk')\nplt.xlabel('Avg Total ESG Risk')\nplt.ylabel('Avg Daily Return')\nplt.axhline(0, color='gray', linestyle='--', linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: Plotting each company’s mean return against its mean ESG score reveals no strong linear trend, points are widely scattered and there’s only a slight downward tilt if you draw a best-fit line. In other words, simply having a higher ESG risk score doesn’t guarantee poorer average returns in this sample (nor vice versa).\n\n# 4) Boxplot of Avg Return by ESG Risk Quartile\ncompany_df['ESG_Quartile'] = pd.qcut(company_df['Total_ESG_Risk'], 4,\n                                     labels=['Low','MidLow','MidHigh','High'])\nplt.figure(figsize=(6,5))\nsns.boxplot(x='ESG_Quartile', y='Return', data=company_df)\nplt.title('4. Avg Return by ESG Risk Quartile')\nplt.xlabel('ESG Risk Quartile')\nplt.ylabel('Avg Daily Return')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: When we split firms into four “Low → High” ESG-risk groups, the median returns are very similar across quartiles, and the interquartile spreads overlap heavily. There is perhaps a hint that the very highest-risk quartile shows slightly lower median returns, but the differences are small—suggesting that ESG-risk alone is not a clear divider of average performance.\n\n# 5) Correlation Heatmap: ESG Sub‐scores vs Avg Return\nsub_esg = esg_df.groupby('Symbol')[['Environmental_Risk',\n                                    'Social_Risk',\n                                    'Governance_Risk']].mean().reset_index()\ndf_corr = (\n    pd.merge(company_df[['Symbol','Total_ESG_Risk','Return']],\n             sub_esg, on='Symbol')\n    .set_index('Symbol')\n    [['Total_ESG_Risk','Environmental_Risk','Social_Risk','Governance_Risk','Return']]\n)\n\nplt.figure(figsize=(6,5))\nsns.heatmap(df_corr.corr(), annot=True, fmt='.2f', linewidths=0.5, cmap='coolwarm')\nplt.title('5. Correlation: ESG Sub-scores & Avg Return')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: In the 5×5 correlation matrix, you’ll see that Environmental_Risk has the most noticeable modest negative correlation with average return, while Social_Risk and Governance_Risk hover near zero correlation. Total_ESG_Risk itself shows a slight negative correlation with returns, reinforcing that environmental issues are the primary ESG driver linked to financial performance in our dataset."
  },
  {
    "objectID": "danl_210_Horn_Ryan_stock_ESG.html#significance-implications",
    "href": "danl_210_Horn_Ryan_stock_ESG.html#significance-implications",
    "title": "Data Analysis Project",
    "section": "6. Significance & Implications",
    "text": "6. Significance & Implications\nOverall, these analyses suggest that while ESG risk scores alone aren’t a silver-bullet predictor of financial performance, they still carry actionable insights:\nFor Investors: Screening out the handful of very high-risk ESG outliers (ESG &gt; 30) can help avoid firms with potential sustainability liabilities. Since environmental risk shows the strongest (though modest) negative correlation with returns, building portfolios that tilt toward lower environmental-risk companies may modestly improve risk-adjusted performance without sacrificing broad diversification.\nFor Corporations: Efforts to reduce overall ESG risk should prioritize environmental initiatives—cleaner operations, emissions controls, and resource efficiency—as these appear most tightly linked to financial health. Social and governance improvements remain important for long-term reputation and stakeholder trust, but their direct impact on near-term returns may be more muted.\nFor Policy Makers: Mandating transparent environmental disclosures and rewarding measurable improvements (e.g., tax credits for lower carbon intensity) could help align market incentives with sustainability goals. Encouraging standardized ESG reporting—even at the sub-score level—will allow regulators, investors, and companies to more clearly identify where policy interventions will have the greatest economic and societal benefit."
  },
  {
    "objectID": "danl_210_Horn_Ryan_stock_ESG.html#references-acknowledgments",
    "href": "danl_210_Horn_Ryan_stock_ESG.html#references-acknowledgments",
    "title": "Data Analysis Project",
    "section": "7. References & Acknowledgments",
    "text": "7. References & Acknowledgments\n\nYahoo Finance: https://finance.yahoo.com/\n\npandas: https://pandas.pydata.org/\n\nseaborn: https://seaborn.pydata.org/\n\nAI & Collaboration: Guided by ChatGPT."
  },
  {
    "objectID": "posts/Python Basics/index.html",
    "href": "posts/Python Basics/index.html",
    "title": "Python Basics",
    "section": "",
    "text": "Lecture 4\nIn lecture 4 we worked with various aspects of python including, values, variables, types, assignments, basic code, and comment styles.\n\n\nLecture 5\nIn lecture 5 we expanded on what we learned in lecture 4 and introduced many more aspects of python. Some of these included Booleans, conditions, and if statements. On top of that we started using slicing methods for strings and lists and many more.\n\n\nClasswork 4\nIn classwork 4 we started with using pythons syntax to solve an algabreaic equation. We then practiced with lists, slicing, while and for loops, and importing libraries."
  },
  {
    "objectID": "posts/Restaurant Inspection/index.html",
    "href": "posts/Restaurant Inspection/index.html",
    "title": "Restaurant Inspections",
    "section": "",
    "text": "Listed here are some analytics related to restaurants in NYC.\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\nAs we can see from the graph above, data from NYC restaurants, critical score varies more as the you move from category A, B, and C.\n\n\n\n\n\n\n\n\n\nThis bar plot shows the top 10 cuisine types based on the number of restaurants.\n\n\n\n\n\n\n\n\n\nThis histogram visualizes the distribution of inspection scores for restaurants."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Thank you for checking this out! This blog is to display some analytics of various data sets, hope you enjoy!"
  },
  {
    "objectID": "posts/NFL 2022 Data/index.html",
    "href": "posts/NFL 2022 Data/index.html",
    "title": "NFL 2022 Data",
    "section": "",
    "text": "Here is some analytics in relation to Data from the 2022 NFL season!"
  },
  {
    "objectID": "posts/NFL 2022 Data/index.html#q2a",
    "href": "posts/NFL 2022 Data/index.html#q2a",
    "title": "NFL 2022 Data",
    "section": "Q2a",
    "text": "Q2a\nIn data.frame, NFL2022_stuffs, remove observations for which values of posteam is missing.\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nAnswer:\n\nNFL2022_stuffs_cleaned &lt;- NFL2022_stuffs %&gt;% \n  filter(!is.na(posteam))\n\nhead(NFL2022_stuffs_cleaned)\n\n  play_id         game_id drive week posteam qtr down half_seconds_remaining\n1      43 2022_01_BAL_NYJ     1    1     NYJ   1   NA                   1800\n2      68 2022_01_BAL_NYJ     1    1     NYJ   1    1                   1796\n3      89 2022_01_BAL_NYJ     1    1     NYJ   1    1                   1769\n4     115 2022_01_BAL_NYJ     1    1     NYJ   1    2                   1765\n5     136 2022_01_BAL_NYJ     1    1     NYJ   1    3                   1741\n6     172 2022_01_BAL_NYJ     1    1     NYJ   1    4                   1733\n  pass        wp\n1    0 0.5462618\n2    0 0.5469690\n3    1 0.5725734\n4    0 0.5545366\n5    1 0.5401673\n6    0 0.4880532"
  },
  {
    "objectID": "posts/NFL 2022 Data/index.html#q2b",
    "href": "posts/NFL 2022 Data/index.html#q2b",
    "title": "NFL 2022 Data",
    "section": "Q2b",
    "text": "Q2b\n-Summarize the mean value of pass for each posteam when all the following conditions hold:\n\nwp is greater than 20% and less than 75%;\ndown is less than or equal to 2; and\nhalf_seconds_remaining is greater than 120\n\nAnswer:\n\nfiltered_data &lt;- NFL2022_stuffs %&gt;%\n  filter(\n    !is.na(posteam),       \n    wp &gt; 0.2 & wp &lt; 0.75,   \n    down &lt;= 2,              \n    half_seconds_remaining &gt; 120  \n  )\n\nsummary_data &lt;- filtered_data %&gt;%\n  group_by(posteam) %&gt;%\n  summarize(mean_pass = mean(pass, na.rm = TRUE))\n\nprint(summary_data)\n\n# A tibble: 32 × 2\n   posteam mean_pass\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 ARI         0.553\n 2 ATL         0.4  \n 3 BAL         0.520\n 4 BUF         0.604\n 5 CAR         0.458\n 6 CHI         0.420\n 7 CIN         0.657\n 8 CLE         0.491\n 9 DAL         0.474\n10 DEN         0.493\n# ℹ 22 more rows"
  },
  {
    "objectID": "posts/NFL 2022 Data/index.html#q2c",
    "href": "posts/NFL 2022 Data/index.html#q2c",
    "title": "NFL 2022 Data",
    "section": "Q2c",
    "text": "Q2c\n-Provide both (1) a ggplot code with geom_point() using the resulting data.frame in Q2b and (2) a simple comments to describe the mean value of pass for each posteam. In the ggplot, reorder the posteam categories based on the mean value of pass in ascending or in descending order.\nAnswer:\n\nlibrary(ggplot2)\nresult &lt;- NFL2022_stuffs %&gt;%\n  filter(\n    wp &gt; 0.2 & wp &lt; 0.75,\n    down &lt;= 2,\n    half_seconds_remaining &gt; 120\n  ) %&gt;%\n  group_by(posteam) %&gt;%\n  summarise(mean_pass = mean(pass, na.rm = TRUE))\n\nresult$posteam &lt;- factor(result$posteam, levels = result$posteam[order(result$mean_pass)])\n\nggplot(result, aes(x = posteam, y = mean_pass)) +\n  geom_point() +\n  labs(title = \"Mean Value of Pass for Each posteam\",\n       x = \"posteam\",\n       y = \"Mean Pass Value\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/NFL 2022 Data/index.html#q2d",
    "href": "posts/NFL 2022 Data/index.html#q2d",
    "title": "NFL 2022 Data",
    "section": "Q2d",
    "text": "Q2d\n\nCreate the data.frame, NFL2022_stuffs_EPA, that includes\n\n\nAll the variables in the data.frame, NFL2022_stuffs;\nThe variables, passer, receiver, and epa, from the data.frame, NFL2022_epa. by joining the two data.frames.\n\n\nIn the resulting data.frame, NFL2022_stuffs_EPA, remove observations with NA in passer.\n\nAnswer:\n\nNFL2022_epa &lt;- read.csv('https://bcdanl.github.io/data/NFL2022_epa.csv')\n\nNFL2022_stuffs_EPA &lt;- left_join(NFL2022_stuffs, NFL2022_epa, by = c(\"play_id\", \"game_id\", \"drive\", \"posteam\"))\n\nNFL2022_stuffs_EPA &lt;- NFL2022_stuffs_EPA %&gt;%\n  filter(!is.na(passer))\n\nhead(NFL2022_stuffs_EPA)\n\n  play_id         game_id drive week posteam qtr down half_seconds_remaining\n1      89 2022_01_BAL_NYJ     1    1     NYJ   1    1                   1769\n2     136 2022_01_BAL_NYJ     1    1     NYJ   1    3                   1741\n3     202 2022_01_BAL_NYJ     2    1     BAL   1    1                   1722\n4     230 2022_01_BAL_NYJ     2    1     BAL   1    2                   1701\n5     301 2022_01_BAL_NYJ     2    1     BAL   1    2                   1579\n6     412 2022_01_BAL_NYJ     3    1     NYJ   1    2                   1451\n  pass        wp   receiver    passer         epa\n1    1 0.5725734  Mi.Carter  J.Flacco -0.49219242\n2    1 0.5401673       &lt;NA&gt;  J.Flacco -2.40220026\n3    1 0.4958201  R.Bateman L.Jackson  0.07512748\n4    1 0.4965942 D.Duvernay L.Jackson -0.10512029\n5    1 0.5067707 D.Duvernay L.Jackson  0.41113183\n6    1 0.5001284    Br.Hall  J.Flacco -0.17972556"
  },
  {
    "objectID": "posts/NFL 2022 Data/index.html#q2e",
    "href": "posts/NFL 2022 Data/index.html#q2e",
    "title": "NFL 2022 Data",
    "section": "Q2e",
    "text": "Q2e\n\nProvide both (1) a single ggplot and (2) a simple comment to describe the NFL weekly trend of weekly mean value of epa for each of the following two passers,\n\n\n“J.Allen”\n“P.Mahomes”\n\nAnswer:\n\nggplot(NFL2022_stuffs_EPA, aes(x = week, y = epa, color = passer, linetype = passer)) +\n  geom_line(linewidth = 1.2) +\n  labs(title = \"NFL Weekly Trend of Mean EPA for Passers\",\n       x = \"Week\",\n       y = \"Mean EPA\") +\n  scale_color_manual(values = c(\"J.Allen\" = \"blue\", \"P.Mahomes\" = \"red\")) +\n  scale_linetype_manual(values = c(\"J.Allen\" = \"solid\", \"P.Mahomes\" = \"dashed\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis line plot illustrates the weekly trend of the mean value of Expected Points Added (EPA) for two excellent passers."
  },
  {
    "objectID": "posts/NFL 2022 Data/index.html#q2f",
    "href": "posts/NFL 2022 Data/index.html#q2f",
    "title": "NFL 2022 Data",
    "section": "Q2f",
    "text": "Q2f\n\nCalculate the difference between the mean value of epa for “J.Allen” the mean value of epa for “P.Mahomes” for each value of week.\n\nAnswer:\n\nepa_difference &lt;- NFL2022_stuffs_EPA %&gt;%\n  group_by(week) %&gt;%\n  summarise(mean_epa_difference = mean(epa[passer == \"J.Allen\"]) - mean(epa[passer == \"P.Mahomes\"]))\n\nprint(epa_difference)\n\n# A tibble: 22 × 2\n    week mean_epa_difference\n   &lt;int&gt;               &lt;dbl&gt;\n 1     1             -0.169 \n 2     2              0.339 \n 3     3             -0.0763\n 4     4             -0.0803\n 5     5              0.325 \n 6     6              0.173 \n 7     7            NaN     \n 8     8            NaN     \n 9     9             -0.304 \n10    10             -0.429 \n# ℹ 12 more rows"
  },
  {
    "objectID": "posts/NFL 2022 Data/index.html#q2g",
    "href": "posts/NFL 2022 Data/index.html#q2g",
    "title": "NFL 2022 Data",
    "section": "Q2g",
    "text": "Q2g\n\nSummarize the resulting data.frame in Q2d, with the following four variables:\n\nposteam: String abbreviation for the team with possession.\npasser: Name of the player who passed a ball to a receiver by initially taking a three-step drop, and backpedaling into the pocket to make a pass. (Mostly, they are quarterbacks.)\nmean_epa: Mean value of epa in 2022 for each passer\nn_pass: Number of observations for each passer\n\nThen find the top 10 NFL passers in 2022 in terms of the mean value of epa, conditioning that n_pass must be greater than or equal to the third quantile level of n_pass.\n\nAnswer:\n\npasser_summary &lt;- NFL2022_stuffs_EPA %&gt;%\n  group_by(posteam, passer) %&gt;%\n  summarise(\n    mean_epa = mean(epa, na.rm = TRUE),\n    n_pass = n()\n  )\n\n`summarise()` has grouped output by 'posteam'. You can override using the\n`.groups` argument.\n\nquantile_threshold &lt;- quantile(passer_summary$n_pass, 0.75)\n\ntop_passers &lt;- passer_summary %&gt;%\n  filter(n_pass &gt;= quantile_threshold) %&gt;%\n  arrange(desc(mean_epa)) %&gt;%\n  slice_head(n = 10)\n\nprint(top_passers)\n\n# A tibble: 29 × 4\n# Groups:   posteam [29]\n   posteam passer     mean_epa n_pass\n   &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;  &lt;int&gt;\n 1 ARI     K.Murray     0.0163    465\n 2 ATL     M.Mariota    0.0251    370\n 3 BAL     L.Jackson    0.0549    398\n 4 BUF     J.Allen      0.172     785\n 5 CHI     J.Fields    -0.0455    469\n 6 CIN     J.Burrow     0.153     854\n 7 CLE     J.Brissett   0.0912    445\n 8 DAL     D.Prescott   0.147     529\n 9 DEN     R.Wilson    -0.0163    609\n10 DET     J.Goff       0.171     661\n# ℹ 19 more rows"
  },
  {
    "objectID": "danl-210-python-basic.html",
    "href": "danl-210-python-basic.html",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')\n\n\n\n\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5\n\n\n\n\n\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\n\n\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\n\n\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-210-python-basic.html#what-is-python",
    "href": "danl-210-python-basic.html#what-is-python",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "danl-210-python-basic.html#variables-and-data-types",
    "href": "danl-210-python-basic.html#variables-and-data-types",
    "title": "Python Basics",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-210-python-basic.html#control-structures",
    "href": "danl-210-python-basic.html#control-structures",
    "title": "Python Basics",
    "section": "",
    "text": "Python supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "danl-210-python-basic.html#functions",
    "href": "danl-210-python-basic.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "A function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "danl-210-python-basic.html#lists-and-dictionaries",
    "href": "danl-210-python-basic.html#lists-and-dictionaries",
    "title": "Python Basics",
    "section": "",
    "text": "A list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "quarto-template.html",
    "href": "quarto-template.html",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "",
    "text": "oj &lt;- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\nnvars &lt;- format(round(ncol(oj), 0), \n                nsmall=0, \n                big.mark=\",\") \nnobs &lt;- format(round(nrow(oj), 0), \n                nsmall=0, \n                big.mark=\",\")\nThe number of variables is 4; the number of observations is 28,947.\nRoses are red\nviolets are blue."
  },
  {
    "objectID": "quarto-template.html#data-summary",
    "href": "quarto-template.html#data-summary",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.1 Data Summary",
    "text": "2.1 Data Summary\n\nSummary statistics (Use skimr::skim())"
  },
  {
    "objectID": "quarto-template.html#data-visualization",
    "href": "quarto-template.html#data-visualization",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.2 Data Visualization",
    "text": "2.2 Data Visualization\n\noj %&gt;% \n  ggplot(aes(x = log(sales), \n             y = log(price),\n             color = brand)) +\n  geom_point(alpha = .1) +\n  geom_smooth(method = lm, se = F) +\n  facet_wrap(.~ad) +\n  theme_bw() +\n  theme(legend.position = 'top')"
  },
  {
    "objectID": "quarto-template.html#data-transformation",
    "href": "quarto-template.html#data-transformation",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.3 Data Transformation",
    "text": "2.3 Data Transformation\n\nob_sum1 &lt;- oj %&gt;% \n  group_by(brand, ad) %&gt;% \n  summarise(sales_tot = sum(sales, na.rm = T),\n            price_mean = round(mean(price, na.rm = T), 2))"
  },
  {
    "objectID": "quarto-template.html#analysis",
    "href": "quarto-template.html#analysis",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.4 Analysis",
    "text": "2.4 Analysis"
  },
  {
    "objectID": "quarto-template.html#quotes",
    "href": "quarto-template.html#quotes",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.5 Quotes",
    "text": "2.5 Quotes\n\nQuote with &gt;\n\n\n“The truth is rarely pure and never simple.”\n— Oscar Wilde"
  },
  {
    "objectID": "quarto-template.html#inserting-figures",
    "href": "quarto-template.html#inserting-figures",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.6 Inserting Figures",
    "text": "2.6 Inserting Figures\nFor a demonstration of a DANL tiger, see Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: DANL Tiger"
  },
  {
    "objectID": "quarto-template.html#inserting-a-html-page",
    "href": "quarto-template.html#inserting-a-html-page",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.7 Inserting a HTML page",
    "text": "2.7 Inserting a HTML page"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan Horn",
    "section": "",
    "text": "I am a second-year Data Analytics major at SUNY Geneseo with a passion for technology and problem-solving. My U.S. Army experience has strengthened my leadership, teamwork, and ability to perform under pressure. I am eager to apply my analytical skills to real-world challenges and connect with professionals to grow in the field."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ryan Horn",
    "section": "Education",
    "text": "Education\nCandidate for Bachelor of Science in Data Analytics at SUNY Geneseo  Geneseo, NY  Aug 2023 - May 2026"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Ryan Horn",
    "section": "Experience",
    "text": "Experience\nUSARNG July 2020 - July 2026"
  },
  {
    "objectID": "danl200-hw5-Horn-Ryan.qmd/danl200-hw5-Horn-Ryan.qmd.html",
    "href": "danl200-hw5-Horn-Ryan.qmd/danl200-hw5-Horn-Ryan.qmd.html",
    "title": "danl200-hw5-Horn-Ryan.qmd",
    "section": "",
    "text": "Below is a link to my GitHub repository.\nhref: https://github.com/rhorn15/rhorn15.github.io\nBelow is a link to my website\nhref:\nhttps://rhorn15.github.io/\nThe answers for Q2 of the homework can be found in the blog section of my website or right here.\nhref: https://rhorn15.github.io/posts/NFL%202022%20Data/"
  },
  {
    "objectID": "danl200-hw5-Horn-Ryan.qmd/danl200-hw5-Horn-Ryan.qmd.html#q1a",
    "href": "danl200-hw5-Horn-Ryan.qmd/danl200-hw5-Horn-Ryan.qmd.html#q1a",
    "title": "danl200-hw5-Horn-Ryan.qmd",
    "section": "",
    "text": "Below is a link to my GitHub repository.\nhref: https://github.com/rhorn15/rhorn15.github.io\nBelow is a link to my website\nhref:\nhttps://rhorn15.github.io/\nThe answers for Q2 of the homework can be found in the blog section of my website or right here.\nhref: https://rhorn15.github.io/posts/NFL%202022%20Data/"
  }
]